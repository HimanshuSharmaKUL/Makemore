{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read all words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build vocabulaty of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.']=0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> m\n",
      "emm ---> a\n",
      "mma ---> .\n",
      "olivia\n",
      "... ---> o\n",
      "..o ---> l\n",
      ".ol ---> i\n",
      "oli ---> v\n",
      "liv ---> i\n",
      "ivi ---> a\n",
      "via ---> .\n",
      "ava\n",
      "... ---> a\n",
      "..a ---> v\n",
      ".av ---> a\n",
      "ava ---> .\n",
      "isabella\n",
      "... ---> i\n",
      "..i ---> s\n",
      ".is ---> a\n",
      "isa ---> b\n",
      "sab ---> e\n",
      "abe ---> l\n",
      "bel ---> l\n",
      "ell ---> a\n",
      "lla ---> .\n",
      "sophia\n",
      "... ---> s\n",
      "..s ---> o\n",
      ".so ---> p\n",
      "sop ---> h\n",
      "oph ---> i\n",
      "phi ---> a\n",
      "hia ---> .\n"
     ]
    }
   ],
   "source": [
    "#build the dataset\n",
    "block_size = 3 #context length: how many characters to look at to predict the next one?\n",
    "X,Y = [],[]\n",
    "for w in words[:5]:\n",
    "    print(w)\n",
    "    context = [0]*block_size\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context = context[1:] + [ix] #crop the first element and apend the new one\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  5, 13],\n",
       "        [ 5, 13, 13],\n",
       "        [13, 13,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 15],\n",
       "        [ 0, 15, 12],\n",
       "        [15, 12,  9],\n",
       "        [12,  9, 22],\n",
       "        [ 9, 22,  9],\n",
       "        [22,  9,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1, 22],\n",
       "        [ 1, 22,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  9],\n",
       "        [ 0,  9, 19],\n",
       "        [ 9, 19,  1],\n",
       "        [19,  1,  2],\n",
       "        [ 1,  2,  5],\n",
       "        [ 2,  5, 12],\n",
       "        [ 5, 12, 12],\n",
       "        [12, 12,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 19],\n",
       "        [ 0, 19, 15],\n",
       "        [19, 15, 16],\n",
       "        [15, 16,  8],\n",
       "        [16,  8,  9],\n",
       "        [ 8,  9,  1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the 'e' is encodded as 5. To encode the examples, we create embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1406,  1.7067])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We create a 2 dimensional tensor of shape (n_words, block_size) containing the integer encoded characters\n",
    "C = torch.randn((27,2)) #the random embedding layer\n",
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1406,  1.7067])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes=27) #one hot encoding of the 5th character\n",
    "#and if we do a dot product of the one hot encoding with the embedding matrix we get the embedding of the 5th character\n",
    "F.one_hot(torch.tensor(5), num_classes=27).float() @ C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e., we can index 5th row from C or we can treat \"one-hot-encoding @ C\" as 1st layer\n",
    "So, we just use embedding tables and discard the one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1406,  1.7067])\n",
      "----\n",
      "tensor([[-1.1406,  1.7067],\n",
      "        [ 1.0376,  1.7113],\n",
      "        [ 0.4232,  0.0789]])\n",
      "----\n",
      "tensor([[-1.1406,  1.7067],\n",
      "        [ 1.0376,  1.7113],\n",
      "        [ 0.4232,  0.0789],\n",
      "        [ 0.4232,  0.0789],\n",
      "        [ 0.4232,  0.0789]])\n",
      "----\n",
      "tensor([[-1.1406,  1.7067],\n",
      "        [ 1.0376,  1.7113],\n",
      "        [ 0.4232,  0.0789]])\n",
      "----\n",
      "torch.Size([32, 3, 2])\n",
      "tensor(1)\n",
      "tensor([1.8286, 0.7084])\n",
      "tensor([1.8286, 0.7084])\n",
      "tensor([1.8286, 0.7084])\n"
     ]
    }
   ],
   "source": [
    "#python indexing is powerful\n",
    "print(C[5])\n",
    "print('-'*4)\n",
    "print(C[[5,6,7]]) #if we provide a list of indexes, we can retrieve mupltiple elements\n",
    "print('-'*4)\n",
    "print(C[[5,6,7,7,7]]) #indexing can retrieve mupltiple elements, and same element multiple times\n",
    "print('-'*4)\n",
    "print(C[torch.tensor([5,6,7])]) \n",
    "print('-'*4)\n",
    "#index can also be 2D\n",
    "#For example: X is 32x3, so we can pass it to C, embedding matrix directly to get the embeddings corresponging ...?\n",
    "print(C[X].shape)\n",
    "print(X[13,2])\n",
    "print(C[X][13,2]) #----(1) also can be written as C[X[13,2]]\n",
    "print(C[X[13,2]]) #----(1a)\n",
    "print(C[1])       #----(2)\n",
    "#(1) & (1a) is the same as (2) because X[13,2] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#basically, our embedddnig is just\n",
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((6,100)) #2 from each of 3 previous 'block_size' context characters\n",
    "b1 = torch.randn(100) #biases for this layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#but we can not do \n",
    "#emb @ W1 + b1 #since emb is 32x3x2 and W1 is 6x100\n",
    "#so, we'll need to do something to make the shapes compatible\n",
    "#we want to retrieve the three context character ebeddings and concatenate them\n",
    "emb[:,0,:].shape #it plucks out the embeddings of the first character/word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so, we do following\n",
    "torch.cat([emb[:,0,:], emb[:,1,:], emb[:,2,:]], 1).shape #concatenating the embeddings of the three context characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.4319, -0.3574],\n",
       "         [ 0.4319, -0.3574],\n",
       "         [ 0.4319, -0.3574],\n",
       "         [-1.1406,  1.7067],\n",
       "         [ 1.2426, -0.9235],\n",
       "         [ 0.4319, -0.3574],\n",
       "         [ 0.4319, -0.3574],\n",
       "         [ 0.4319, -0.3574],\n",
       "         [-0.4959,  0.0249],\n",
       "         [-0.9457,  1.2776],\n",
       "         [ 0.7457,  0.0992],\n",
       "         [ 0.6225,  1.2407],\n",
       "         [ 0.4319, -0.3574],\n",
       "         [ 0.4319, -0.3574],\n",
       "         [ 0.4319, -0.3574],\n",
       "         [ 1.8286,  0.7084],\n",
       "         [ 0.4319, -0.3574],\n",
       "         [ 0.4319, -0.3574],\n",
       "         [ 0.4319, -0.3574],\n",
       "         [ 0.7457,  0.0992],\n",
       "         [ 0.0982, -1.6248],\n",
       "         [ 1.8286,  0.7084],\n",
       "         [ 0.5046,  0.0502],\n",
       "         [-1.1406,  1.7067],\n",
       "         [-0.9457,  1.2776],\n",
       "         [ 0.4319, -0.3574],\n",
       "         [ 0.4319, -0.3574],\n",
       "         [ 0.4319, -0.3574],\n",
       "         [ 0.0982, -1.6248],\n",
       "         [-0.4959,  0.0249],\n",
       "         [-1.1105,  1.6384],\n",
       "         [ 0.5265,  0.3940]]),\n",
       " tensor([[ 0.4319, -0.3574],\n",
       "         [ 0.4319, -0.3574],\n",
       "         [-1.1406,  1.7067],\n",
       "         [ 1.2426, -0.9235],\n",
       "         [ 1.2426, -0.9235],\n",
       "         [ 0.4319, -0.3574],\n",
       "         [ 0.4319, -0.3574],\n",
       "         [-0.4959,  0.0249],\n",
       "         [-0.9457,  1.2776],\n",
       "         [ 0.7457,  0.0992],\n",
       "         [ 0.6225,  1.2407],\n",
       "         [ 0.7457,  0.0992],\n",
       "         [ 0.4319, -0.3574],\n",
       "         [ 0.4319, -0.3574],\n",
       "         [ 1.8286,  0.7084],\n",
       "         [ 0.6225,  1.2407],\n",
       "         [ 0.4319, -0.3574],\n",
       "         [ 0.4319, -0.3574],\n",
       "         [ 0.7457,  0.0992],\n",
       "         [ 0.0982, -1.6248],\n",
       "         [ 1.8286,  0.7084],\n",
       "         [ 0.5046,  0.0502],\n",
       "         [-1.1406,  1.7067],\n",
       "         [-0.9457,  1.2776],\n",
       "         [-0.9457,  1.2776],\n",
       "         [ 0.4319, -0.3574],\n",
       "         [ 0.4319, -0.3574],\n",
       "         [ 0.0982, -1.6248],\n",
       "         [-0.4959,  0.0249],\n",
       "         [-1.1105,  1.6384],\n",
       "         [ 0.5265,  0.3940],\n",
       "         [ 0.7457,  0.0992]]),\n",
       " tensor([[ 0.4319, -0.3574],\n",
       "         [-1.1406,  1.7067],\n",
       "         [ 1.2426, -0.9235],\n",
       "         [ 1.2426, -0.9235],\n",
       "         [ 1.8286,  0.7084],\n",
       "         [ 0.4319, -0.3574],\n",
       "         [-0.4959,  0.0249],\n",
       "         [-0.9457,  1.2776],\n",
       "         [ 0.7457,  0.0992],\n",
       "         [ 0.6225,  1.2407],\n",
       "         [ 0.7457,  0.0992],\n",
       "         [ 1.8286,  0.7084],\n",
       "         [ 0.4319, -0.3574],\n",
       "         [ 1.8286,  0.7084],\n",
       "         [ 0.6225,  1.2407],\n",
       "         [ 1.8286,  0.7084],\n",
       "         [ 0.4319, -0.3574],\n",
       "         [ 0.7457,  0.0992],\n",
       "         [ 0.0982, -1.6248],\n",
       "         [ 1.8286,  0.7084],\n",
       "         [ 0.5046,  0.0502],\n",
       "         [-1.1406,  1.7067],\n",
       "         [-0.9457,  1.2776],\n",
       "         [-0.9457,  1.2776],\n",
       "         [ 1.8286,  0.7084],\n",
       "         [ 0.4319, -0.3574],\n",
       "         [ 0.0982, -1.6248],\n",
       "         [-0.4959,  0.0249],\n",
       "         [-1.1105,  1.6384],\n",
       "         [ 0.5265,  0.3940],\n",
       "         [ 0.7457,  0.0992],\n",
       "         [ 1.8286,  0.7084]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unbind(emb, 1) #this will give us a list of 3 tensors, each of shape 32x2, exactly equivalant to \"[emb[:,0,:], emb[:,1,:], emb[:,2,:]]\"\"makemore part 2.ipynb\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4319, -0.3574,  0.4319, -0.3574,  0.4319, -0.3574],\n",
       "        [ 0.4319, -0.3574,  0.4319, -0.3574, -1.1406,  1.7067],\n",
       "        [ 0.4319, -0.3574, -1.1406,  1.7067,  1.2426, -0.9235],\n",
       "        [-1.1406,  1.7067,  1.2426, -0.9235,  1.2426, -0.9235],\n",
       "        [ 1.2426, -0.9235,  1.2426, -0.9235,  1.8286,  0.7084],\n",
       "        [ 0.4319, -0.3574,  0.4319, -0.3574,  0.4319, -0.3574],\n",
       "        [ 0.4319, -0.3574,  0.4319, -0.3574, -0.4959,  0.0249],\n",
       "        [ 0.4319, -0.3574, -0.4959,  0.0249, -0.9457,  1.2776],\n",
       "        [-0.4959,  0.0249, -0.9457,  1.2776,  0.7457,  0.0992],\n",
       "        [-0.9457,  1.2776,  0.7457,  0.0992,  0.6225,  1.2407],\n",
       "        [ 0.7457,  0.0992,  0.6225,  1.2407,  0.7457,  0.0992],\n",
       "        [ 0.6225,  1.2407,  0.7457,  0.0992,  1.8286,  0.7084],\n",
       "        [ 0.4319, -0.3574,  0.4319, -0.3574,  0.4319, -0.3574],\n",
       "        [ 0.4319, -0.3574,  0.4319, -0.3574,  1.8286,  0.7084],\n",
       "        [ 0.4319, -0.3574,  1.8286,  0.7084,  0.6225,  1.2407],\n",
       "        [ 1.8286,  0.7084,  0.6225,  1.2407,  1.8286,  0.7084],\n",
       "        [ 0.4319, -0.3574,  0.4319, -0.3574,  0.4319, -0.3574],\n",
       "        [ 0.4319, -0.3574,  0.4319, -0.3574,  0.7457,  0.0992],\n",
       "        [ 0.4319, -0.3574,  0.7457,  0.0992,  0.0982, -1.6248],\n",
       "        [ 0.7457,  0.0992,  0.0982, -1.6248,  1.8286,  0.7084],\n",
       "        [ 0.0982, -1.6248,  1.8286,  0.7084,  0.5046,  0.0502],\n",
       "        [ 1.8286,  0.7084,  0.5046,  0.0502, -1.1406,  1.7067],\n",
       "        [ 0.5046,  0.0502, -1.1406,  1.7067, -0.9457,  1.2776],\n",
       "        [-1.1406,  1.7067, -0.9457,  1.2776, -0.9457,  1.2776],\n",
       "        [-0.9457,  1.2776, -0.9457,  1.2776,  1.8286,  0.7084],\n",
       "        [ 0.4319, -0.3574,  0.4319, -0.3574,  0.4319, -0.3574],\n",
       "        [ 0.4319, -0.3574,  0.4319, -0.3574,  0.0982, -1.6248],\n",
       "        [ 0.4319, -0.3574,  0.0982, -1.6248, -0.4959,  0.0249],\n",
       "        [ 0.0982, -1.6248, -0.4959,  0.0249, -1.1105,  1.6384],\n",
       "        [-0.4959,  0.0249, -1.1105,  1.6384,  0.5265,  0.3940],\n",
       "        [-1.1105,  1.6384,  0.5265,  0.3940,  0.7457,  0.0992],\n",
       "        [ 0.5265,  0.3940,  0.7457,  0.0992,  1.8286,  0.7084]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so, we do:\n",
    "torch.cat(torch.unbind(emb, 1), 1) #slicing and concatenating the embeddings of the three context characters this way \n",
    "                                   #is more flecible than hardcoding the no. of context chars/words (3 above) as we did above\n",
    "                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can also use .view() to do the same thing which is even more efficient\n",
    "a = torch.arange(18)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.view(3,2,3) or\n",
    "# a.view(9,2) or\n",
    "# a.view(2,9) or etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\AppData\\Local\\Temp\\ipykernel_18300\\4032053941.py:2: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  a.storage()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9\n",
       " 10\n",
       " 11\n",
       " 12\n",
       " 13\n",
       " 14\n",
       " 15\n",
       " 16\n",
       " 17\n",
       "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 18]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is possible since pytorch stores the numbers in a simple 1D vector\n",
    "a.storage()\n",
    "#so, when we call .view(), no memory is copied, just the shape and stride information is changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So, now insteadd of \"torch.cat(torch.unbind(emb, 1), 1) \", \n",
    "#we do:\n",
    "# emb.view(32,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so, the code becomes:\n",
    "h = torch.tanh(emb.view(32,6) @ W1 + b1)\n",
    "h.shape\n",
    "\n",
    "#now, if we do emb.view(-1,6), then pytorch will automatically infer the first dimension to be 32, or whatebver is the size of the first dimension of emb\n",
    "#so, the code becomes:\n",
    "h = torch.tanh(emb.view(-1,6) @ W1 + b1)\n",
    "\n",
    "#Caution about '+' operator broadcasting!\n",
    "#we've\n",
    "#32,100 #emb.view(-1,6) @ W1\n",
    "#   100 #b1\n",
    "#so, broadcasting will do:\n",
    "#32,100\n",
    "# 1,100 #i.e., add 1 more dimention to b1. And each row of b1 will be added to each row of the result of emb.view(-1,6) @ W1\n",
    "#Here, correct thing would be happening, as same bias vector will be added to the rows\n",
    "\n",
    "#torch.cat is inefficient since it creates new memories and new tensors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100,27)) #27 is the no. of characters in our vocab, #100 is the no. of hidden units in previous layer\n",
    "b2 = torch.randn(27) #biases for this layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = h@W2 + b2 #output of W2 layer\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = logits.exp() #to get fake counts\n",
    "prob = counts / counts.sum(1, keepdims=True) #normalizing the counts to get probabilities\n",
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.8226e-07, 1.8257e-03, 5.3622e-06, 3.7332e-05, 2.3064e-16, 3.0600e-03,\n",
       "        3.7274e-06, 3.1741e-12, 4.2422e-08, 5.5294e-11, 4.8401e-09, 2.6763e-12,\n",
       "        1.8752e-06, 4.5004e-05, 1.1623e-05, 5.2608e-14, 8.6591e-10, 2.3419e-04,\n",
       "        1.6275e-10, 9.2767e-13, 9.4700e-15, 2.2960e-11, 1.4364e-13, 6.7789e-07,\n",
       "        4.0280e-11, 5.0272e-06, 9.5522e-09, 1.5012e-05, 9.4591e-07, 3.0820e-14,\n",
       "        1.0558e-08, 5.7285e-13])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now, just like previous noteebook, we index each row and pluck out the probab assigned to next character as given in Y\n",
    "#we've an iterator from 0 to 31 -> torch.arange(32)\n",
    "prob[torch.arange(32), Y]  #iterators over the rows of prob and plucks out the probab assigned to next character as given in Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19.1263)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#these probablities of next characters should be nearer to 1, then the model is doing well\n",
    "#so, we can take the log of these probabilities and average them up to get the loss, and then -ve it to get negative log likelihood\n",
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss #we have to minimize this loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.Size([32]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape #dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) #for reproducibility\n",
    "C = torch.randn((27,2), generator=g) #embedding layer\n",
    "\n",
    "W1 = torch.randn((6,100), generator=g) #first layer\n",
    "b1 = torch.randn(100, generator=g) #biases for first layer\n",
    "\n",
    "W2 = torch.randn((100,27), generator=g) #second layer\n",
    "b2 = torch.randn(27, generator=g) #biases for second layer\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2] #all the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) #total no. of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Forward pass\n",
    "emb = C[X] #embedding layer #32x3x2\n",
    "\n",
    "h = torch.tanh(emb.view(-1,6) @ W1 + b1) #first layer #32x100\n",
    "\n",
    "logits = h@W2 + b2 #output of W2 layer #32x27\n",
    "# counts = logits.exp() #to get fake counts\n",
    "# prob = counts / counts.sum(1, keepdims=True) #normalizing the counts to get probabilities\n",
    "# loss = -prob[torch.arange(32), Y].log().mean() #loss\n",
    "#instead of above manner of calculating loss, we can use F.cross_entropy() which does the same thing\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss\n",
    "#we use F.cross_entropy() 'cause:\n",
    "#1. pytorch wont create new intermediate tensors, and instead clusters them and use fused kernels\n",
    "#2. expressions of derivatives simplify which is used by pytorch\n",
    "#3. under the hood, cross-entropy can be more numericaly more well behaved: for example, if logits have a high +ve value, then \n",
    "#   taking ann exp of it can run out of dynamic range of floating point values. Ex: logit has a 100, then e^100 is out of range\n",
    "#   but cross_entropy internally subtracts the biggest number from logits, to offset it timestamp: @37:45\n",
    "#   awesome explaination\n",
    "\n",
    "#bakcward pass\n",
    "for p in parameters:\n",
    "    p.grad = None #setting gradients to 0 before each backward pass\n",
    "loss.backward()\n",
    "\n",
    "#update\n",
    "for p in parameters:\n",
    "    p.data += -0.1*p.grad #updating the parameters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True #enabling gradients for all parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.76971435546875\n",
      "13.656400680541992\n",
      "11.298768997192383\n",
      "9.4524564743042\n",
      "7.984262466430664\n",
      "6.891321182250977\n",
      "6.100014686584473\n",
      "5.452036380767822\n",
      "4.898151874542236\n",
      "4.414664268493652\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    #Forward pass\n",
    "    emb = C[X] #embedding layer #32x3x2\n",
    "    h = torch.tanh(emb.view(-1,6) @ W1 + b1) #first layer #32x100\n",
    "    logits = h@W2 + b2 #output of W2 layer #32x27\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print(loss.item())\n",
    "    \n",
    "    #bakcward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None #setting gradients to 0 before each backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    #update\n",
    "    for p in parameters:\n",
    "        p.data += -0.1*p.grad #updating the parameters\n",
    "\n",
    "#since we only have 32 examples, and 3481 params, it is very easy to overfit the data and achieve low error rates\n",
    "#so, we can use a larger dataset to train the model\n",
    "# we still are not able to achieve 0 loss, since ... needs to predict a, o, i etc i.e. ... ko multiple things predict karni hai\n",
    "# but jahan jahan unique combinations hain previous 3 characters ki, wahan near to 0 loss achieve kar rhe hain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So let us do this for all words and not just 5\n",
    "#build the dataset\n",
    "block_size = 3 #context length: how many characters to look at to predict the next one?\n",
    "X,Y = [],[]\n",
    "for w in words: #all words\n",
    "    \n",
    "    context = [0]*block_size\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        # print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context = context[1:] + [ix] #crop the first element and apend the new one\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.Size([228146]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape #dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) #for reproducibility\n",
    "C = torch.randn((27,2), generator=g) #embedding layer\n",
    "\n",
    "W1 = torch.randn((6,100), generator=g) #first layer\n",
    "b1 = torch.randn(100, generator=g) #biases for first layer\n",
    "\n",
    "W2 = torch.randn((100,27), generator=g) #second layer\n",
    "b2 = torch.randn(27, generator=g) #biases for second layer\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2] #all the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) #total no. of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True #enabling gradients for all parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.722684860229492\n",
      "17.80501365661621\n",
      "15.122965812683105\n",
      "15.761963844299316\n",
      "11.688470840454102\n",
      "11.920717239379883\n",
      "15.058136940002441\n",
      "11.585692405700684\n",
      "12.898177146911621\n",
      "10.77206039428711\n",
      "12.597578048706055\n",
      "10.017439842224121\n",
      "10.154825210571289\n",
      "8.948321342468262\n",
      "11.433053970336914\n",
      "11.569074630737305\n",
      "9.36916732788086\n",
      "7.798447608947754\n",
      "8.66457748413086\n",
      "8.724126815795898\n",
      "7.289283275604248\n",
      "8.376447677612305\n",
      "7.311673641204834\n",
      "7.42368745803833\n",
      "6.9834699630737305\n",
      "7.075383186340332\n",
      "7.5329084396362305\n",
      "7.430334568023682\n",
      "5.531008720397949\n",
      "7.091148376464844\n",
      "6.237879753112793\n",
      "6.119830131530762\n",
      "6.990893840789795\n",
      "8.585159301757812\n",
      "7.845841407775879\n",
      "5.922115325927734\n",
      "6.849554061889648\n",
      "5.938353061676025\n",
      "6.442837715148926\n",
      "4.078384876251221\n",
      "5.787971496582031\n",
      "6.617045879364014\n",
      "5.197267532348633\n",
      "4.058233261108398\n",
      "6.0693464279174805\n",
      "5.1775665283203125\n",
      "7.14966344833374\n",
      "4.33341121673584\n",
      "4.22559928894043\n",
      "5.316624164581299\n",
      "7.456387519836426\n",
      "4.514888763427734\n",
      "5.502017498016357\n",
      "5.026523590087891\n",
      "5.907541275024414\n",
      "4.9185590744018555\n",
      "4.969006061553955\n",
      "5.385982990264893\n",
      "4.232009410858154\n",
      "6.292393207550049\n",
      "5.284913063049316\n",
      "4.823166370391846\n",
      "5.566194534301758\n",
      "4.669311046600342\n",
      "4.21328067779541\n",
      "5.12045431137085\n",
      "4.161154747009277\n",
      "4.567470550537109\n",
      "4.291911602020264\n",
      "5.437402725219727\n",
      "4.702456951141357\n",
      "3.5992069244384766\n",
      "4.219570636749268\n",
      "4.324046611785889\n",
      "4.617512226104736\n",
      "4.850399017333984\n",
      "4.7339253425598145\n",
      "4.4416351318359375\n",
      "4.357207775115967\n",
      "3.8045530319213867\n",
      "3.851680040359497\n",
      "4.983422756195068\n",
      "3.80796480178833\n",
      "3.505523204803467\n",
      "4.7876996994018555\n",
      "4.005710601806641\n",
      "4.647600173950195\n",
      "3.7883083820343018\n",
      "3.633373737335205\n",
      "4.490501403808594\n",
      "3.8605761528015137\n",
      "3.2987520694732666\n",
      "4.698502540588379\n",
      "3.183438301086426\n",
      "4.24937105178833\n",
      "3.609555244445801\n",
      "3.764254093170166\n",
      "3.436674118041992\n",
      "6.000914096832275\n",
      "3.5711350440979004\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    \n",
    "    #it'll be much faster in minibatches\n",
    "    #minibatch construct\n",
    "    ix = torch.randint(0,X.shape[0],(32,)) #randomly sample 32 numbers from 0 to 228146\n",
    "\n",
    "\n",
    "    #Forward pass\n",
    "    emb = C[X[ix]] #embedding layer #First pick only ix indexes from X, and then based on X[ix], pick only X[ix] indexes from C #32x3x2\n",
    "    h = torch.tanh(emb.view(-1,6) @ W1 + b1) #first layer #32x100\n",
    "    logits = h@W2 + b2 #output of W2 layer #32x27\n",
    "    loss = F.cross_entropy(logits, Y[ix]) #ix also indexes from Y, so that we can calculate loss for only those 32 examples\n",
    "    print(loss.item())\n",
    "    \n",
    "    #bakcward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None #setting gradients to 0 before each backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    #update\n",
    "    for p in parameters:\n",
    "        p.data += -0.1*p.grad #updating the parameters\n",
    "\n",
    "#In Minibatches, the quality of gradients is not as good as in full batch, but it is much faster\n",
    "#since it is better to make many steps with aproximate gradients than to make few steps with exact gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Above fwd and backward pass is being done for each of 228146 examples\n",
    "#so, instead of passing each example individually, we can do this fwd and backward pass in batches\n",
    "#we randomly sample from the examples and then pass them in batches\n",
    "torch.randint(0, X.shape[0], (32,)) #generate numbers bw 0 and 228156, and randomly sample 32 numbers from the dataset\n",
    "                                    #these are the indexes of the examples we'll use in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "        0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0013, 0.0013, 0.0013,\n",
       "        0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0014,\n",
       "        0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
       "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
       "        0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
       "        0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017,\n",
       "        0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0019,\n",
       "        0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0020, 0.0020,\n",
       "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0021, 0.0021, 0.0021, 0.0021,\n",
       "        0.0021, 0.0021, 0.0021, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "        0.0022, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0024, 0.0024,\n",
       "        0.0024, 0.0024, 0.0024, 0.0024, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
       "        0.0025, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0027, 0.0027, 0.0027,\n",
       "        0.0027, 0.0027, 0.0027, 0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0029,\n",
       "        0.0029, 0.0029, 0.0029, 0.0029, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n",
       "        0.0031, 0.0031, 0.0031, 0.0031, 0.0032, 0.0032, 0.0032, 0.0032, 0.0032,\n",
       "        0.0033, 0.0033, 0.0033, 0.0033, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
       "        0.0035, 0.0035, 0.0035, 0.0035, 0.0036, 0.0036, 0.0036, 0.0036, 0.0037,\n",
       "        0.0037, 0.0037, 0.0037, 0.0038, 0.0038, 0.0038, 0.0039, 0.0039, 0.0039,\n",
       "        0.0039, 0.0040, 0.0040, 0.0040, 0.0040, 0.0041, 0.0041, 0.0041, 0.0042,\n",
       "        0.0042, 0.0042, 0.0042, 0.0043, 0.0043, 0.0043, 0.0044, 0.0044, 0.0044,\n",
       "        0.0045, 0.0045, 0.0045, 0.0045, 0.0046, 0.0046, 0.0046, 0.0047, 0.0047,\n",
       "        0.0047, 0.0048, 0.0048, 0.0048, 0.0049, 0.0049, 0.0049, 0.0050, 0.0050,\n",
       "        0.0050, 0.0051, 0.0051, 0.0051, 0.0052, 0.0052, 0.0053, 0.0053, 0.0053,\n",
       "        0.0054, 0.0054, 0.0054, 0.0055, 0.0055, 0.0056, 0.0056, 0.0056, 0.0057,\n",
       "        0.0057, 0.0058, 0.0058, 0.0058, 0.0059, 0.0059, 0.0060, 0.0060, 0.0060,\n",
       "        0.0061, 0.0061, 0.0062, 0.0062, 0.0062, 0.0063, 0.0063, 0.0064, 0.0064,\n",
       "        0.0065, 0.0065, 0.0066, 0.0066, 0.0067, 0.0067, 0.0067, 0.0068, 0.0068,\n",
       "        0.0069, 0.0069, 0.0070, 0.0070, 0.0071, 0.0071, 0.0072, 0.0072, 0.0073,\n",
       "        0.0073, 0.0074, 0.0074, 0.0075, 0.0075, 0.0076, 0.0076, 0.0077, 0.0077,\n",
       "        0.0078, 0.0079, 0.0079, 0.0080, 0.0080, 0.0081, 0.0081, 0.0082, 0.0082,\n",
       "        0.0083, 0.0084, 0.0084, 0.0085, 0.0085, 0.0086, 0.0086, 0.0087, 0.0088,\n",
       "        0.0088, 0.0089, 0.0090, 0.0090, 0.0091, 0.0091, 0.0092, 0.0093, 0.0093,\n",
       "        0.0094, 0.0095, 0.0095, 0.0096, 0.0097, 0.0097, 0.0098, 0.0099, 0.0099,\n",
       "        0.0100, 0.0101, 0.0101, 0.0102, 0.0103, 0.0104, 0.0104, 0.0105, 0.0106,\n",
       "        0.0106, 0.0107, 0.0108, 0.0109, 0.0109, 0.0110, 0.0111, 0.0112, 0.0112,\n",
       "        0.0113, 0.0114, 0.0115, 0.0116, 0.0116, 0.0117, 0.0118, 0.0119, 0.0120,\n",
       "        0.0121, 0.0121, 0.0122, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0127,\n",
       "        0.0128, 0.0129, 0.0130, 0.0131, 0.0132, 0.0133, 0.0134, 0.0135, 0.0136,\n",
       "        0.0137, 0.0137, 0.0138, 0.0139, 0.0140, 0.0141, 0.0142, 0.0143, 0.0144,\n",
       "        0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.0150, 0.0151, 0.0152, 0.0154,\n",
       "        0.0155, 0.0156, 0.0157, 0.0158, 0.0159, 0.0160, 0.0161, 0.0162, 0.0163,\n",
       "        0.0165, 0.0166, 0.0167, 0.0168, 0.0169, 0.0170, 0.0171, 0.0173, 0.0174,\n",
       "        0.0175, 0.0176, 0.0178, 0.0179, 0.0180, 0.0181, 0.0182, 0.0184, 0.0185,\n",
       "        0.0186, 0.0188, 0.0189, 0.0190, 0.0192, 0.0193, 0.0194, 0.0196, 0.0197,\n",
       "        0.0198, 0.0200, 0.0201, 0.0202, 0.0204, 0.0205, 0.0207, 0.0208, 0.0210,\n",
       "        0.0211, 0.0212, 0.0214, 0.0215, 0.0217, 0.0218, 0.0220, 0.0221, 0.0223,\n",
       "        0.0225, 0.0226, 0.0228, 0.0229, 0.0231, 0.0232, 0.0234, 0.0236, 0.0237,\n",
       "        0.0239, 0.0241, 0.0242, 0.0244, 0.0246, 0.0247, 0.0249, 0.0251, 0.0253,\n",
       "        0.0254, 0.0256, 0.0258, 0.0260, 0.0261, 0.0263, 0.0265, 0.0267, 0.0269,\n",
       "        0.0271, 0.0273, 0.0274, 0.0276, 0.0278, 0.0280, 0.0282, 0.0284, 0.0286,\n",
       "        0.0288, 0.0290, 0.0292, 0.0294, 0.0296, 0.0298, 0.0300, 0.0302, 0.0304,\n",
       "        0.0307, 0.0309, 0.0311, 0.0313, 0.0315, 0.0317, 0.0320, 0.0322, 0.0324,\n",
       "        0.0326, 0.0328, 0.0331, 0.0333, 0.0335, 0.0338, 0.0340, 0.0342, 0.0345,\n",
       "        0.0347, 0.0350, 0.0352, 0.0354, 0.0357, 0.0359, 0.0362, 0.0364, 0.0367,\n",
       "        0.0369, 0.0372, 0.0375, 0.0377, 0.0380, 0.0382, 0.0385, 0.0388, 0.0390,\n",
       "        0.0393, 0.0396, 0.0399, 0.0401, 0.0404, 0.0407, 0.0410, 0.0413, 0.0416,\n",
       "        0.0418, 0.0421, 0.0424, 0.0427, 0.0430, 0.0433, 0.0436, 0.0439, 0.0442,\n",
       "        0.0445, 0.0448, 0.0451, 0.0455, 0.0458, 0.0461, 0.0464, 0.0467, 0.0471,\n",
       "        0.0474, 0.0477, 0.0480, 0.0484, 0.0487, 0.0491, 0.0494, 0.0497, 0.0501,\n",
       "        0.0504, 0.0508, 0.0511, 0.0515, 0.0518, 0.0522, 0.0526, 0.0529, 0.0533,\n",
       "        0.0537, 0.0540, 0.0544, 0.0548, 0.0552, 0.0556, 0.0559, 0.0563, 0.0567,\n",
       "        0.0571, 0.0575, 0.0579, 0.0583, 0.0587, 0.0591, 0.0595, 0.0599, 0.0604,\n",
       "        0.0608, 0.0612, 0.0616, 0.0621, 0.0625, 0.0629, 0.0634, 0.0638, 0.0642,\n",
       "        0.0647, 0.0651, 0.0656, 0.0660, 0.0665, 0.0670, 0.0674, 0.0679, 0.0684,\n",
       "        0.0688, 0.0693, 0.0698, 0.0703, 0.0708, 0.0713, 0.0718, 0.0723, 0.0728,\n",
       "        0.0733, 0.0738, 0.0743, 0.0748, 0.0753, 0.0758, 0.0764, 0.0769, 0.0774,\n",
       "        0.0780, 0.0785, 0.0790, 0.0796, 0.0802, 0.0807, 0.0813, 0.0818, 0.0824,\n",
       "        0.0830, 0.0835, 0.0841, 0.0847, 0.0853, 0.0859, 0.0865, 0.0871, 0.0877,\n",
       "        0.0883, 0.0889, 0.0895, 0.0901, 0.0908, 0.0914, 0.0920, 0.0927, 0.0933,\n",
       "        0.0940, 0.0946, 0.0953, 0.0959, 0.0966, 0.0973, 0.0979, 0.0986, 0.0993,\n",
       "        0.1000, 0.1007, 0.1014, 0.1021, 0.1028, 0.1035, 0.1042, 0.1050, 0.1057,\n",
       "        0.1064, 0.1072, 0.1079, 0.1087, 0.1094, 0.1102, 0.1109, 0.1117, 0.1125,\n",
       "        0.1133, 0.1140, 0.1148, 0.1156, 0.1164, 0.1172, 0.1181, 0.1189, 0.1197,\n",
       "        0.1205, 0.1214, 0.1222, 0.1231, 0.1239, 0.1248, 0.1256, 0.1265, 0.1274,\n",
       "        0.1283, 0.1292, 0.1301, 0.1310, 0.1319, 0.1328, 0.1337, 0.1346, 0.1356,\n",
       "        0.1365, 0.1374, 0.1384, 0.1394, 0.1403, 0.1413, 0.1423, 0.1433, 0.1443,\n",
       "        0.1453, 0.1463, 0.1473, 0.1483, 0.1493, 0.1504, 0.1514, 0.1525, 0.1535,\n",
       "        0.1546, 0.1557, 0.1567, 0.1578, 0.1589, 0.1600, 0.1611, 0.1623, 0.1634,\n",
       "        0.1645, 0.1657, 0.1668, 0.1680, 0.1691, 0.1703, 0.1715, 0.1727, 0.1739,\n",
       "        0.1751, 0.1763, 0.1775, 0.1788, 0.1800, 0.1812, 0.1825, 0.1838, 0.1850,\n",
       "        0.1863, 0.1876, 0.1889, 0.1902, 0.1916, 0.1929, 0.1942, 0.1956, 0.1969,\n",
       "        0.1983, 0.1997, 0.2010, 0.2024, 0.2038, 0.2053, 0.2067, 0.2081, 0.2096,\n",
       "        0.2110, 0.2125, 0.2140, 0.2154, 0.2169, 0.2184, 0.2200, 0.2215, 0.2230,\n",
       "        0.2246, 0.2261, 0.2277, 0.2293, 0.2309, 0.2325, 0.2341, 0.2357, 0.2373,\n",
       "        0.2390, 0.2406, 0.2423, 0.2440, 0.2457, 0.2474, 0.2491, 0.2508, 0.2526,\n",
       "        0.2543, 0.2561, 0.2579, 0.2597, 0.2615, 0.2633, 0.2651, 0.2669, 0.2688,\n",
       "        0.2707, 0.2725, 0.2744, 0.2763, 0.2783, 0.2802, 0.2821, 0.2841, 0.2861,\n",
       "        0.2880, 0.2900, 0.2921, 0.2941, 0.2961, 0.2982, 0.3002, 0.3023, 0.3044,\n",
       "        0.3065, 0.3087, 0.3108, 0.3130, 0.3151, 0.3173, 0.3195, 0.3217, 0.3240,\n",
       "        0.3262, 0.3285, 0.3308, 0.3331, 0.3354, 0.3377, 0.3400, 0.3424, 0.3448,\n",
       "        0.3472, 0.3496, 0.3520, 0.3544, 0.3569, 0.3594, 0.3619, 0.3644, 0.3669,\n",
       "        0.3695, 0.3720, 0.3746, 0.3772, 0.3798, 0.3825, 0.3851, 0.3878, 0.3905,\n",
       "        0.3932, 0.3959, 0.3987, 0.4014, 0.4042, 0.4070, 0.4098, 0.4127, 0.4155,\n",
       "        0.4184, 0.4213, 0.4243, 0.4272, 0.4302, 0.4331, 0.4362, 0.4392, 0.4422,\n",
       "        0.4453, 0.4484, 0.4515, 0.4546, 0.4578, 0.4610, 0.4642, 0.4674, 0.4706,\n",
       "        0.4739, 0.4772, 0.4805, 0.4838, 0.4872, 0.4906, 0.4940, 0.4974, 0.5008,\n",
       "        0.5043, 0.5078, 0.5113, 0.5149, 0.5185, 0.5221, 0.5257, 0.5293, 0.5330,\n",
       "        0.5367, 0.5404, 0.5442, 0.5479, 0.5517, 0.5556, 0.5594, 0.5633, 0.5672,\n",
       "        0.5712, 0.5751, 0.5791, 0.5831, 0.5872, 0.5913, 0.5954, 0.5995, 0.6036,\n",
       "        0.6078, 0.6120, 0.6163, 0.6206, 0.6249, 0.6292, 0.6336, 0.6380, 0.6424,\n",
       "        0.6469, 0.6513, 0.6559, 0.6604, 0.6650, 0.6696, 0.6743, 0.6789, 0.6837,\n",
       "        0.6884, 0.6932, 0.6980, 0.7028, 0.7077, 0.7126, 0.7176, 0.7225, 0.7275,\n",
       "        0.7326, 0.7377, 0.7428, 0.7480, 0.7531, 0.7584, 0.7636, 0.7689, 0.7743,\n",
       "        0.7796, 0.7850, 0.7905, 0.7960, 0.8015, 0.8071, 0.8127, 0.8183, 0.8240,\n",
       "        0.8297, 0.8355, 0.8412, 0.8471, 0.8530, 0.8589, 0.8648, 0.8708, 0.8769,\n",
       "        0.8830, 0.8891, 0.8953, 0.9015, 0.9077, 0.9140, 0.9204, 0.9268, 0.9332,\n",
       "        0.9397, 0.9462, 0.9528, 0.9594, 0.9660, 0.9727, 0.9795, 0.9863, 0.9931,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#But, how to select proper learning rate?\n",
    "#we can use torch.linspace() to generate a range of learning rates and then plot the loss vs learning rate\n",
    "lre = torch.linspace(0.001, 1, 1000) #generate 1000 learning rates from 0.001 to 1 in a linear space\n",
    "#but, linear space se better hai ki us intervel me exponentially placed ho\n",
    "lre = torch.linspace(-3, 0, 1000)\n",
    "lrs = 10**lre #convert the linearly spaced learning rates to exponentially spaced learning rates, 10^0 is 1\n",
    "lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.0000, -2.9970, -2.9940, -2.9910, -2.9880, -2.9850, -2.9820, -2.9790,\n",
       "        -2.9760, -2.9730, -2.9700, -2.9670, -2.9640, -2.9610, -2.9580, -2.9550,\n",
       "        -2.9520, -2.9489, -2.9459, -2.9429, -2.9399, -2.9369, -2.9339, -2.9309,\n",
       "        -2.9279, -2.9249, -2.9219, -2.9189, -2.9159, -2.9129, -2.9099, -2.9069,\n",
       "        -2.9039, -2.9009, -2.8979, -2.8949, -2.8919, -2.8889, -2.8859, -2.8829,\n",
       "        -2.8799, -2.8769, -2.8739, -2.8709, -2.8679, -2.8649, -2.8619, -2.8589,\n",
       "        -2.8559, -2.8529, -2.8498, -2.8468, -2.8438, -2.8408, -2.8378, -2.8348,\n",
       "        -2.8318, -2.8288, -2.8258, -2.8228, -2.8198, -2.8168, -2.8138, -2.8108,\n",
       "        -2.8078, -2.8048, -2.8018, -2.7988, -2.7958, -2.7928, -2.7898, -2.7868,\n",
       "        -2.7838, -2.7808, -2.7778, -2.7748, -2.7718, -2.7688, -2.7658, -2.7628,\n",
       "        -2.7598, -2.7568, -2.7538, -2.7508, -2.7477, -2.7447, -2.7417, -2.7387,\n",
       "        -2.7357, -2.7327, -2.7297, -2.7267, -2.7237, -2.7207, -2.7177, -2.7147,\n",
       "        -2.7117, -2.7087, -2.7057, -2.7027, -2.6997, -2.6967, -2.6937, -2.6907,\n",
       "        -2.6877, -2.6847, -2.6817, -2.6787, -2.6757, -2.6727, -2.6697, -2.6667,\n",
       "        -2.6637, -2.6607, -2.6577, -2.6547, -2.6517, -2.6486, -2.6456, -2.6426,\n",
       "        -2.6396, -2.6366, -2.6336, -2.6306, -2.6276, -2.6246, -2.6216, -2.6186,\n",
       "        -2.6156, -2.6126, -2.6096, -2.6066, -2.6036, -2.6006, -2.5976, -2.5946,\n",
       "        -2.5916, -2.5886, -2.5856, -2.5826, -2.5796, -2.5766, -2.5736, -2.5706,\n",
       "        -2.5676, -2.5646, -2.5616, -2.5586, -2.5556, -2.5526, -2.5495, -2.5465,\n",
       "        -2.5435, -2.5405, -2.5375, -2.5345, -2.5315, -2.5285, -2.5255, -2.5225,\n",
       "        -2.5195, -2.5165, -2.5135, -2.5105, -2.5075, -2.5045, -2.5015, -2.4985,\n",
       "        -2.4955, -2.4925, -2.4895, -2.4865, -2.4835, -2.4805, -2.4775, -2.4745,\n",
       "        -2.4715, -2.4685, -2.4655, -2.4625, -2.4595, -2.4565, -2.4535, -2.4505,\n",
       "        -2.4474, -2.4444, -2.4414, -2.4384, -2.4354, -2.4324, -2.4294, -2.4264,\n",
       "        -2.4234, -2.4204, -2.4174, -2.4144, -2.4114, -2.4084, -2.4054, -2.4024,\n",
       "        -2.3994, -2.3964, -2.3934, -2.3904, -2.3874, -2.3844, -2.3814, -2.3784,\n",
       "        -2.3754, -2.3724, -2.3694, -2.3664, -2.3634, -2.3604, -2.3574, -2.3544,\n",
       "        -2.3514, -2.3483, -2.3453, -2.3423, -2.3393, -2.3363, -2.3333, -2.3303,\n",
       "        -2.3273, -2.3243, -2.3213, -2.3183, -2.3153, -2.3123, -2.3093, -2.3063,\n",
       "        -2.3033, -2.3003, -2.2973, -2.2943, -2.2913, -2.2883, -2.2853, -2.2823,\n",
       "        -2.2793, -2.2763, -2.2733, -2.2703, -2.2673, -2.2643, -2.2613, -2.2583,\n",
       "        -2.2553, -2.2523, -2.2492, -2.2462, -2.2432, -2.2402, -2.2372, -2.2342,\n",
       "        -2.2312, -2.2282, -2.2252, -2.2222, -2.2192, -2.2162, -2.2132, -2.2102,\n",
       "        -2.2072, -2.2042, -2.2012, -2.1982, -2.1952, -2.1922, -2.1892, -2.1862,\n",
       "        -2.1832, -2.1802, -2.1772, -2.1742, -2.1712, -2.1682, -2.1652, -2.1622,\n",
       "        -2.1592, -2.1562, -2.1532, -2.1502, -2.1471, -2.1441, -2.1411, -2.1381,\n",
       "        -2.1351, -2.1321, -2.1291, -2.1261, -2.1231, -2.1201, -2.1171, -2.1141,\n",
       "        -2.1111, -2.1081, -2.1051, -2.1021, -2.0991, -2.0961, -2.0931, -2.0901,\n",
       "        -2.0871, -2.0841, -2.0811, -2.0781, -2.0751, -2.0721, -2.0691, -2.0661,\n",
       "        -2.0631, -2.0601, -2.0571, -2.0541, -2.0511, -2.0480, -2.0450, -2.0420,\n",
       "        -2.0390, -2.0360, -2.0330, -2.0300, -2.0270, -2.0240, -2.0210, -2.0180,\n",
       "        -2.0150, -2.0120, -2.0090, -2.0060, -2.0030, -2.0000, -1.9970, -1.9940,\n",
       "        -1.9910, -1.9880, -1.9850, -1.9820, -1.9790, -1.9760, -1.9730, -1.9700,\n",
       "        -1.9670, -1.9640, -1.9610, -1.9580, -1.9550, -1.9520, -1.9489, -1.9459,\n",
       "        -1.9429, -1.9399, -1.9369, -1.9339, -1.9309, -1.9279, -1.9249, -1.9219,\n",
       "        -1.9189, -1.9159, -1.9129, -1.9099, -1.9069, -1.9039, -1.9009, -1.8979,\n",
       "        -1.8949, -1.8919, -1.8889, -1.8859, -1.8829, -1.8799, -1.8769, -1.8739,\n",
       "        -1.8709, -1.8679, -1.8649, -1.8619, -1.8589, -1.8559, -1.8529, -1.8498,\n",
       "        -1.8468, -1.8438, -1.8408, -1.8378, -1.8348, -1.8318, -1.8288, -1.8258,\n",
       "        -1.8228, -1.8198, -1.8168, -1.8138, -1.8108, -1.8078, -1.8048, -1.8018,\n",
       "        -1.7988, -1.7958, -1.7928, -1.7898, -1.7868, -1.7838, -1.7808, -1.7778,\n",
       "        -1.7748, -1.7718, -1.7688, -1.7658, -1.7628, -1.7598, -1.7568, -1.7538,\n",
       "        -1.7508, -1.7477, -1.7447, -1.7417, -1.7387, -1.7357, -1.7327, -1.7297,\n",
       "        -1.7267, -1.7237, -1.7207, -1.7177, -1.7147, -1.7117, -1.7087, -1.7057,\n",
       "        -1.7027, -1.6997, -1.6967, -1.6937, -1.6907, -1.6877, -1.6847, -1.6817,\n",
       "        -1.6787, -1.6757, -1.6727, -1.6697, -1.6667, -1.6637, -1.6607, -1.6577,\n",
       "        -1.6547, -1.6517, -1.6486, -1.6456, -1.6426, -1.6396, -1.6366, -1.6336,\n",
       "        -1.6306, -1.6276, -1.6246, -1.6216, -1.6186, -1.6156, -1.6126, -1.6096,\n",
       "        -1.6066, -1.6036, -1.6006, -1.5976, -1.5946, -1.5916, -1.5886, -1.5856,\n",
       "        -1.5826, -1.5796, -1.5766, -1.5736, -1.5706, -1.5676, -1.5646, -1.5616,\n",
       "        -1.5586, -1.5556, -1.5526, -1.5495, -1.5465, -1.5435, -1.5405, -1.5375,\n",
       "        -1.5345, -1.5315, -1.5285, -1.5255, -1.5225, -1.5195, -1.5165, -1.5135,\n",
       "        -1.5105, -1.5075, -1.5045, -1.5015, -1.4985, -1.4955, -1.4925, -1.4895,\n",
       "        -1.4865, -1.4835, -1.4805, -1.4775, -1.4745, -1.4715, -1.4685, -1.4655,\n",
       "        -1.4625, -1.4595, -1.4565, -1.4535, -1.4505, -1.4474, -1.4444, -1.4414,\n",
       "        -1.4384, -1.4354, -1.4324, -1.4294, -1.4264, -1.4234, -1.4204, -1.4174,\n",
       "        -1.4144, -1.4114, -1.4084, -1.4054, -1.4024, -1.3994, -1.3964, -1.3934,\n",
       "        -1.3904, -1.3874, -1.3844, -1.3814, -1.3784, -1.3754, -1.3724, -1.3694,\n",
       "        -1.3664, -1.3634, -1.3604, -1.3574, -1.3544, -1.3514, -1.3483, -1.3453,\n",
       "        -1.3423, -1.3393, -1.3363, -1.3333, -1.3303, -1.3273, -1.3243, -1.3213,\n",
       "        -1.3183, -1.3153, -1.3123, -1.3093, -1.3063, -1.3033, -1.3003, -1.2973,\n",
       "        -1.2943, -1.2913, -1.2883, -1.2853, -1.2823, -1.2793, -1.2763, -1.2733,\n",
       "        -1.2703, -1.2673, -1.2643, -1.2613, -1.2583, -1.2553, -1.2523, -1.2492,\n",
       "        -1.2462, -1.2432, -1.2402, -1.2372, -1.2342, -1.2312, -1.2282, -1.2252,\n",
       "        -1.2222, -1.2192, -1.2162, -1.2132, -1.2102, -1.2072, -1.2042, -1.2012,\n",
       "        -1.1982, -1.1952, -1.1922, -1.1892, -1.1862, -1.1832, -1.1802, -1.1772,\n",
       "        -1.1742, -1.1712, -1.1682, -1.1652, -1.1622, -1.1592, -1.1562, -1.1532,\n",
       "        -1.1502, -1.1471, -1.1441, -1.1411, -1.1381, -1.1351, -1.1321, -1.1291,\n",
       "        -1.1261, -1.1231, -1.1201, -1.1171, -1.1141, -1.1111, -1.1081, -1.1051,\n",
       "        -1.1021, -1.0991, -1.0961, -1.0931, -1.0901, -1.0871, -1.0841, -1.0811,\n",
       "        -1.0781, -1.0751, -1.0721, -1.0691, -1.0661, -1.0631, -1.0601, -1.0571,\n",
       "        -1.0541, -1.0511, -1.0480, -1.0450, -1.0420, -1.0390, -1.0360, -1.0330,\n",
       "        -1.0300, -1.0270, -1.0240, -1.0210, -1.0180, -1.0150, -1.0120, -1.0090,\n",
       "        -1.0060, -1.0030, -1.0000, -0.9970, -0.9940, -0.9910, -0.9880, -0.9850,\n",
       "        -0.9820, -0.9790, -0.9760, -0.9730, -0.9700, -0.9670, -0.9640, -0.9610,\n",
       "        -0.9580, -0.9550, -0.9520, -0.9489, -0.9459, -0.9429, -0.9399, -0.9369,\n",
       "        -0.9339, -0.9309, -0.9279, -0.9249, -0.9219, -0.9189, -0.9159, -0.9129,\n",
       "        -0.9099, -0.9069, -0.9039, -0.9009, -0.8979, -0.8949, -0.8919, -0.8889,\n",
       "        -0.8859, -0.8829, -0.8799, -0.8769, -0.8739, -0.8709, -0.8679, -0.8649,\n",
       "        -0.8619, -0.8589, -0.8559, -0.8529, -0.8498, -0.8468, -0.8438, -0.8408,\n",
       "        -0.8378, -0.8348, -0.8318, -0.8288, -0.8258, -0.8228, -0.8198, -0.8168,\n",
       "        -0.8138, -0.8108, -0.8078, -0.8048, -0.8018, -0.7988, -0.7958, -0.7928,\n",
       "        -0.7898, -0.7868, -0.7838, -0.7808, -0.7778, -0.7748, -0.7718, -0.7688,\n",
       "        -0.7658, -0.7628, -0.7598, -0.7568, -0.7538, -0.7508, -0.7477, -0.7447,\n",
       "        -0.7417, -0.7387, -0.7357, -0.7327, -0.7297, -0.7267, -0.7237, -0.7207,\n",
       "        -0.7177, -0.7147, -0.7117, -0.7087, -0.7057, -0.7027, -0.6997, -0.6967,\n",
       "        -0.6937, -0.6907, -0.6877, -0.6847, -0.6817, -0.6787, -0.6757, -0.6727,\n",
       "        -0.6697, -0.6667, -0.6637, -0.6607, -0.6577, -0.6547, -0.6517, -0.6486,\n",
       "        -0.6456, -0.6426, -0.6396, -0.6366, -0.6336, -0.6306, -0.6276, -0.6246,\n",
       "        -0.6216, -0.6186, -0.6156, -0.6126, -0.6096, -0.6066, -0.6036, -0.6006,\n",
       "        -0.5976, -0.5946, -0.5916, -0.5886, -0.5856, -0.5826, -0.5796, -0.5766,\n",
       "        -0.5736, -0.5706, -0.5676, -0.5646, -0.5616, -0.5586, -0.5556, -0.5526,\n",
       "        -0.5495, -0.5465, -0.5435, -0.5405, -0.5375, -0.5345, -0.5315, -0.5285,\n",
       "        -0.5255, -0.5225, -0.5195, -0.5165, -0.5135, -0.5105, -0.5075, -0.5045,\n",
       "        -0.5015, -0.4985, -0.4955, -0.4925, -0.4895, -0.4865, -0.4835, -0.4805,\n",
       "        -0.4775, -0.4745, -0.4715, -0.4685, -0.4655, -0.4625, -0.4595, -0.4565,\n",
       "        -0.4535, -0.4505, -0.4474, -0.4444, -0.4414, -0.4384, -0.4354, -0.4324,\n",
       "        -0.4294, -0.4264, -0.4234, -0.4204, -0.4174, -0.4144, -0.4114, -0.4084,\n",
       "        -0.4054, -0.4024, -0.3994, -0.3964, -0.3934, -0.3904, -0.3874, -0.3844,\n",
       "        -0.3814, -0.3784, -0.3754, -0.3724, -0.3694, -0.3664, -0.3634, -0.3604,\n",
       "        -0.3574, -0.3544, -0.3514, -0.3483, -0.3453, -0.3423, -0.3393, -0.3363,\n",
       "        -0.3333, -0.3303, -0.3273, -0.3243, -0.3213, -0.3183, -0.3153, -0.3123,\n",
       "        -0.3093, -0.3063, -0.3033, -0.3003, -0.2973, -0.2943, -0.2913, -0.2883,\n",
       "        -0.2853, -0.2823, -0.2793, -0.2763, -0.2733, -0.2703, -0.2673, -0.2643,\n",
       "        -0.2613, -0.2583, -0.2553, -0.2523, -0.2492, -0.2462, -0.2432, -0.2402,\n",
       "        -0.2372, -0.2342, -0.2312, -0.2282, -0.2252, -0.2222, -0.2192, -0.2162,\n",
       "        -0.2132, -0.2102, -0.2072, -0.2042, -0.2012, -0.1982, -0.1952, -0.1922,\n",
       "        -0.1892, -0.1862, -0.1832, -0.1802, -0.1772, -0.1742, -0.1712, -0.1682,\n",
       "        -0.1652, -0.1622, -0.1592, -0.1562, -0.1532, -0.1502, -0.1471, -0.1441,\n",
       "        -0.1411, -0.1381, -0.1351, -0.1321, -0.1291, -0.1261, -0.1231, -0.1201,\n",
       "        -0.1171, -0.1141, -0.1111, -0.1081, -0.1051, -0.1021, -0.0991, -0.0961,\n",
       "        -0.0931, -0.0901, -0.0871, -0.0841, -0.0811, -0.0781, -0.0751, -0.0721,\n",
       "        -0.0691, -0.0661, -0.0631, -0.0601, -0.0571, -0.0541, -0.0511, -0.0480,\n",
       "        -0.0450, -0.0420, -0.0390, -0.0360, -0.0330, -0.0300, -0.0270, -0.0240,\n",
       "        -0.0210, -0.0180, -0.0150, -0.0120, -0.0090, -0.0060, -0.0030,  0.0000])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.176069498062134\n"
     ]
    }
   ],
   "source": [
    "#self adaptive learning rate, low in the beginning and then high\n",
    "lri = []\n",
    "lossi = []\n",
    "for i in range(10000):\n",
    "    \n",
    "    #it'll be much faster in minibatches\n",
    "    #minibatch construct\n",
    "    ix = torch.randint(0,X.shape[0],(32,)) #randomly sample 32 numbers from 0 to 228146\n",
    "\n",
    "\n",
    "    #Forward pass\n",
    "    emb = C[X[ix]] #embedding layer #First pick only ix indexes from X, and then based on X[ix], pick only X[ix] indexes from C #32x3x2\n",
    "    h = torch.tanh(emb.view(-1,6) @ W1 + b1) #first layer #32x100\n",
    "    logits = h@W2 + b2 #output of W2 layer #32x27\n",
    "    loss = F.cross_entropy(logits, Y[ix]) #ix also indexes from Y, so that we can calculate loss for only those 32 examples\n",
    "    # print(loss.item())\n",
    "    \n",
    "    #bakcward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None #setting gradients to 0 before each backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    #update\n",
    "    # lr = lrs[i]\n",
    "    lr = 0.01 #in the later stages, we can use a still smaller learning rate, it'll be like learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data += -lr*p.grad\n",
    "    \n",
    "    #track stats\n",
    "    # lri.append(lre[i])\n",
    "    # lossi.append(loss.item())\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20932844090>]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lri, lossi) #i.e. lre around -1 is good , i.e 10^-1 = 0.1 is good learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
